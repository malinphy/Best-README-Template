{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "BERT_MLM.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyPhKwZxk8X4i0IkvOE1VgSQ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/malinphy/Best-README-Template/blob/master/BERT_MLM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAB0auYbkBL_"
      },
      "source": [
        "#### This notebook was created to test Bert Masked Encoder to be used for Masked language model and the Seq4Rec using MultiHeadAttention Layer\n",
        "# 80% Replace with a [MASK] token: For 80% of the selected inputs the token is replaced with a [MASK] token similar to the classic cloze tests mentioned earlier.\n",
        "# 10% Replace with an incorrect word: For 10% of the selected inputs, the token is replaced by another randomly selected word whose only requirement is that itâ€™s different from the selected token.\n",
        "# 10% Replace with the correct word: The remaining 10% of the time the selected token is simply replaced with the correct token.\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### PARAMETER_LIST \n",
        "OUTPUT_LEN =256\n",
        "SEQUENCE_LEN =  256\n",
        "MAX_LEN = SEQUENCE_LEN\n",
        "EMBED_DIM = 128\n",
        "VOCAB_SIZE = 30000\n",
        "TOKEN_NUM = 30000\n",
        "N_NEURONS = 128\n",
        "NUM_HEADS = 8\n",
        "KEY_DIM = 128\n",
        "NUM_ATT_LAYER = 1\n",
        "SPECIAL_TOKENS=[\"[MASK]\"]"
      ],
      "metadata": {
        "id": "aI24KdzBzXSa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_url = 'https://raw.githubusercontent.com/malinphy/datasets/main/IMDB_sent/IMDB%20Dataset.csv'\n",
        "df = pd.read_csv(data_url)"
      ],
      "metadata": {
        "id": "U61fUZkZzf2R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def custom_standardization(input_data):\n",
        "    lowercase = str(input_data).lower()\n",
        "    s = re.sub(\"<br />\", \" \",lowercase)\n",
        "    s = re.sub('\\x96|\\x85|\\xe3',' ',s) \n",
        "    out = re.sub('[%s]' % re.escape(string.punctuation), '', s)\n",
        "    return out\n",
        "\n",
        "df['review'] = df['review'].apply(custom_standardization)"
      ],
      "metadata": {
        "id": "-anwWQ9Izh2X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.head(3)"
      ],
      "metadata": {
        "id": "dFrekQLGi_6-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vectorize_layer = TextVectorization(\n",
        "        max_tokens=VOCAB_SIZE,\n",
        "        output_mode=\"int\",\n",
        "        # standardize=custom_standardization,\n",
        "        output_sequence_length=SEQUENCE_LEN,\n",
        "    )\n",
        "vectorize_layer.adapt((df['review']))"
      ],
      "metadata": {
        "id": "RZJ0k5EIi_1J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "vocab = vectorize_layer.get_vocabulary()\n",
        "vocab = vocab[2 : VOCAB_SIZE - len(SPECIAL_TOKENS)] + [\"[mask]\"]\n",
        "vectorize_layer.set_vocabulary(vocab)\n",
        "mask_token_id = vectorize_layer([\"[mask]\"]).numpy()[0][0]"
      ],
      "metadata": {
        "id": "FDDCygEqi_sf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoded_texts = vectorize_layer(df['review'])"
      ],
      "metadata": {
        "id": "N4vimrSGjE1l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_masked_input_and_labels(encoded_texts):\n",
        "    # 15% BERT masking\n",
        "    inp_mask = np.random.rand(*encoded_texts.shape) < 0.15\n",
        "    # Do not mask special tokens\n",
        "    inp_mask[encoded_texts <= 2] = False\n",
        "    # Set targets to -1 by default, it means ignore\n",
        "    labels = -1 * np.ones(encoded_texts.shape, dtype=int)\n",
        "    # Set labels for masked tokens\n",
        "    labels[inp_mask] = encoded_texts[inp_mask]\n",
        "\n",
        "    # Prepare input\n",
        "    encoded_texts_masked = np.copy(encoded_texts)\n",
        "    # Set input to [MASK] which is the last token for the 90% of tokens\n",
        "    # This means leaving 10% unchanged\n",
        "    inp_mask_2mask = inp_mask & (np.random.rand(*encoded_texts.shape) < 0.90)\n",
        "    encoded_texts_masked[\n",
        "        inp_mask_2mask\n",
        "    ] = mask_token_id  # mask token is the last in the dict\n",
        "\n",
        "    # Set 10% to a random token\n",
        "    inp_mask_2random = inp_mask_2mask & (np.random.rand(*encoded_texts.shape) < 1 / 9)\n",
        "    encoded_texts_masked[inp_mask_2random] = np.random.randint(\n",
        "        3, mask_token_id, inp_mask_2random.sum()\n",
        "    )\n",
        "\n",
        "    # Prepare sample_weights to pass to .fit() method\n",
        "    sample_weights = np.ones(labels.shape)\n",
        "    sample_weights[labels == -1] = 0\n",
        "\n",
        "    # y_labels would be same as encoded_texts i.e input tokens\n",
        "    y_labels = np.copy(encoded_texts)\n",
        "\n",
        "    return encoded_texts_masked, y_labels, sample_weights"
      ],
      "metadata": {
        "id": "8-wr2R_ujEyI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bir, iki, uc =get_masked_input_and_labels(encoded_texts)"
      ],
      "metadata": {
        "id": "IJN-8m5DjEuN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_pos_encoding_matrix(max_len, d_emb):\n",
        "    pos_enc = np.array(\n",
        "        [\n",
        "            [pos / np.power(10000, 2 * (j // 2) / d_emb) for j in range(d_emb)]\n",
        "            if pos != 0\n",
        "            else np.zeros(d_emb)\n",
        "            for pos in range(max_len)\n",
        "        ]\n",
        "    )\n",
        "    pos_enc[1:, 0::2] = np.sin(pos_enc[1:, 0::2])  # dim 2i\n",
        "    pos_enc[1:, 1::2] = np.cos(pos_enc[1:, 1::2])  # dim 2i+1\n",
        "    return pos_enc"
      ],
      "metadata": {
        "id": "5iyHtM0DjEqR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "first_input = tf.keras.Input(shape = (SEQUENCE_LEN,))\n",
        "embedding_layer = Embedding(VOCAB_SIZE,EMBED_DIM)(first_input)\n",
        "position_embeddings = layers.Embedding(\n",
        "        input_dim=SEQUENCE_LEN,\n",
        "        output_dim=EMBED_DIM,\n",
        "        weights=[get_pos_encoding_matrix(SEQUENCE_LEN, EMBED_DIM)],\n",
        "        name=\"position_embedding\",\n",
        "    )(tf.range(start=0, limit=SEQUENCE_LEN, delta=1))\n",
        "# total_embs = word_embeddings + get_pos_encoding_matrix(SEQUENCE_LEN,EMBED_DIM)\n",
        "# total_embs = word_embeddings + position_embeddings\n",
        "x = embedding_layer + position_embeddings\n",
        "for i in range(NUM_ATT_LAYER):\n",
        "  mha = MultiHeadAttention(num_heads = NUM_HEADS, key_dim = 16)(x,x,x)\n",
        "  norm_1 = LayerNormalization(epsilon=1e-6)(mha+x)\n",
        "\n",
        "  seq_model = tf.keras.Sequential([\n",
        "                                   Dense(N_NEURONS, activation = 'relu'),\n",
        "                                   Dense(EMBED_DIM)\n",
        "  ])\n",
        "\n",
        "  seq_layer = seq_model(norm_1)\n",
        "  norm_2 =LayerNormalization(epsilon=1e-6)(seq_layer+norm_1)\n",
        "  x = norm_2\n",
        "\n",
        "top_layer = Dense(VOCAB_SIZE+1, activation='relu', name = 'selection_layer')(x)\n",
        "attention_model = tf.keras.Model(inputs = first_input, outputs = top_layer)\n",
        "tf.keras.utils.plot_model(\n",
        "    attention_model,\n",
        "    to_file='model.png', show_shapes=False, show_dtype=False,\n",
        "    show_layer_names=True, rankdir='TB', expand_nested=False, dpi=96,\n",
        "    layer_range=None, show_layer_activations=False\n",
        ")\n",
        "\n",
        "attention_model.compile(\n",
        "    loss = 'SparseCategoricalCrossentropy',\n",
        "    optimizer = tf.keras.optimizers.Adam(\n",
        "    learning_rate=0.001,\n",
        "    #  beta_1=0.9, beta_2=0.999, epsilon=1e-07\n",
        "     ),\n",
        "    metrics = ['accuracy']\n",
        "\n",
        ")\n",
        "\n",
        "attention_model.fit(bir,iki, epochs = 12, batch_size = 30)\n",
        "id2token = dict(enumerate(vectorize_layer.get_vocabulary()))\n",
        "token2id = {y: x for x, y in id2token.items()}\n",
        "sample_tokens = vectorize_layer([\"I have watched this [mask] and it was awesome\"])\n",
        "prediction = attention_model.predict(sample_tokens)\n",
        "mask_token_positions = np.where(sample_tokens == mask_token_id)\n",
        "mask_token_position = mask_token_positions[0]\n",
        "mask_prediction = prediction[0][mask_token_position]\n",
        "token = np.argmax(mask_prediction)\n",
        "id2token[token]"
      ],
      "metadata": {
        "id": "8qjBpSB-jEmW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "zwBJvhrPjEiw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "YHQski67jEcb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}